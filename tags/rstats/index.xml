<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rstats | Stefano Coretta</title>
    <link>https://stefanocoretta.github.io/tags/rstats/</link>
      <atom:link href="https://stefanocoretta.github.io/tags/rstats/index.xml" rel="self" type="application/rss+xml" />
    <description>rstats</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>© 2017-2020 Stefano Coretta</copyright><lastBuildDate>Mon, 17 Jun 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>rstats</title>
      <link>https://stefanocoretta.github.io/tags/rstats/</link>
    </image>
    
    <item>
      <title>Plotting prior distributions with ggplot2</title>
      <link>https://stefanocoretta.github.io/post/plot-prior-distributions-with-ggplot2/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://stefanocoretta.github.io/post/plot-prior-distributions-with-ggplot2/</guid>
      <description>
&lt;script src=&#34;https://stefanocoretta.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The choice of priors is a fundamental step of the Bayesian inference process. &lt;span class=&#34;citation&#34;&gt;Vasishth et al. (2018)&lt;/span&gt; recommend plotting the chosen priors to see if they are reasonable.&lt;/p&gt;
&lt;p&gt;In this post I will show how to easily plot prior distributions in &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;ggplot2&lt;/a&gt; (which is part of the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let’s load the tidyverse first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.4     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.4.0     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_minimal()) # I just like this theme :)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;plotting-your-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting your priors&lt;/h2&gt;
&lt;p&gt;Let’s start with a simple normal prior with &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; = 0 and &lt;em&gt;sd&lt;/em&gt; = 1.&lt;/p&gt;
&lt;p&gt;The plot is initialised with an empty call to &lt;code&gt;ggplot()&lt;/code&gt;.
As aesthetics, you only need to specify the range of &lt;em&gt;x&lt;/em&gt; values in &lt;code&gt;aes()&lt;/code&gt;.
Here, we use &lt;code&gt;c(-4, 4)&lt;/code&gt;, meaning that the &lt;em&gt;x&lt;/em&gt;-axis of this plot will have these limits.
For a normal distribution, it is useful to set the limits as the mean ± 4 times the standard deviation (this ensures all the distribution is shown).&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;ggplot2::stat_function()&lt;/code&gt; allows us to specify a distribution family with the &lt;code&gt;fun&lt;/code&gt; argument.
This arguments takes the density function (the R functions of the form &lt;em&gt;dxxx&lt;/em&gt;) of the chosen distribution family, so for the normal (Gaussian) distribution we use &lt;code&gt;dnorm()&lt;/code&gt;.
The argument &lt;code&gt;n&lt;/code&gt; specifies the number of points along which to calculate the distribution (here &lt;code&gt;101&lt;/code&gt;), while &lt;code&gt;args&lt;/code&gt; takes a list with the parameters of the distribution (here the mean &lt;code&gt;0&lt;/code&gt; and standard deviation &lt;code&gt;1&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = tibble(x = -4:4), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(1)) +
  labs(title = &amp;quot;Normal (Gaussian) distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2019-06-17-plot-prior-distributions-with-ggplot2_files/figure-html/normal-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A beta prior will be bounded between 0 and 1, so we can specify that in &lt;code&gt;aes()&lt;/code&gt;.
The beta distribution has two arguments, &lt;code&gt;shape1&lt;/code&gt; and &lt;code&gt;shape2&lt;/code&gt; (here &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = tibble(x = 0:1), aes(x)) +
  stat_function(fun = dbeta, n = 101, args = list(2, 5)) +
  labs(title = &amp;quot;Beta distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2019-06-17-plot-prior-distributions-with-ggplot2_files/figure-html/beta-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another common distribution is the Cauchy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = tibble(x = -10:10), aes(x)) +
  stat_function(fun = dcauchy, n = 201, args = list(-2, 1)) +
  labs(title = &amp;quot;Cauchy distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2019-06-17-plot-prior-distributions-with-ggplot2_files/figure-html/cauchy-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Poisson distribution can be plotted by changing the type of &lt;code&gt;geom&lt;/code&gt; and using an &lt;code&gt;n&lt;/code&gt; that creates only integers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the range 0:20 includes 21 integers, so n = 21
ggplot(data = tibble(x = 0:20), aes(x)) +
  stat_function(fun = dpois, n = 21, args = list(4), geom = &amp;quot;point&amp;quot;) +
  labs(title = &amp;quot;Poisson distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2019-06-17-plot-prior-distributions-with-ggplot2_files/figure-html/poisson-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course any family with a corresponding &lt;em&gt;dxxx&lt;/em&gt; function can be plotted (see &lt;code&gt;?Distributions&lt;/code&gt; and package-provided families).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-vasishth2018&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Vasishth, Shravan, M. Beckman, B. Nicenboim, Fangfang Li, and Eun Jong Kong. 2018. &lt;span&gt;“Bayesian Data Analysis in the Phonetic Sciences: &lt;span&gt;A&lt;/span&gt; Tutorial Introduction.”&lt;/span&gt; &lt;em&gt;Journal of Phonetics&lt;/em&gt; 71: 147–61.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plotting tongue contours with ggplot2</title>
      <link>https://stefanocoretta.github.io/post/plotting-tongue-contours-with-ggplot2/</link>
      <pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://stefanocoretta.github.io/post/plotting-tongue-contours-with-ggplot2/</guid>
      <description>
&lt;script src=&#34;https://stefanocoretta.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When plotting tongue contours data obtained from ultrasound tongue imaging in &lt;code&gt;R&lt;/code&gt; using &lt;code&gt;ggplot2&lt;/code&gt;, a common option to smooth over the individual contours and show the general pattern is to use &lt;code&gt;geom_smooth(methood = &#34;loess&#34;)&lt;/code&gt;. However, as I will show in this post, in certain cases this method leads to very disorted contours. Such distortion is more or less always present, although at a lower degree in less extreme cases.&lt;/p&gt;
&lt;p&gt;To show the shortcomings of using &lt;code&gt;geom_smooth()&lt;/code&gt; and present a viable alternative, we’ll be using ultrasound tongue imaging data from one speaker (me). This dataset includes tongue contours from within the closure of the conosonants /t, d/ preceeded by /a, o, u/. The dataset looks like this (some columns dropped):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select(tongue_data, rec_date, fan_line, X, Y, word, vowel, c2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,239 x 7
##    rec_date            fan_line     X     Y word  vowel c2   
##    &amp;lt;chr&amp;gt;                  &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
##  1 29/11/2016 15:10:52        6  37.4  9.25 pada  a     d    
##  2 29/11/2016 15:21:30        6  38.6 13.1  pada  a     d    
##  3 29/11/2016 15:10:52        7  34.4 10.3  pada  a     d    
##  4 29/11/2016 15:11:03        7  34.3  9.81 pata  a     t    
##  5 29/11/2016 15:11:14        7  34.6 11.0  podo  o     d    
##  6 29/11/2016 15:13:39        7  34.3  9.65 pada  a     d    
##  7 29/11/2016 15:16:05        7  34.8 11.5  pada  a     d    
##  8 29/11/2016 15:17:07        7  34.5 10.5  putu  u     t    
##  9 29/11/2016 15:19:45        7  34.3  9.64 putu  u     t    
## 10 29/11/2016 15:21:30        7  35.4 13.8  pada  a     d    
## # … with 1,229 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;rec_date&lt;/code&gt; is the date and time of recording. Each observed tongue contour has a unique &lt;code&gt;rec_date&lt;/code&gt; (this will come in handy later). &lt;code&gt;fan_line&lt;/code&gt; is the number of the line in the fan coordinate system used by Articulate Assistant Advanced (which I used to record the data). &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; are the horizontal and vertical position of each point on the contour. The unit is millimeters. &lt;code&gt;word&lt;/code&gt;, &lt;code&gt;vowel&lt;/code&gt; and &lt;code&gt;c2&lt;/code&gt; are self-explanatory.&lt;/p&gt;
&lt;p&gt;Let’s start by plotting the smoothed contours by vowel and consonant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tongue_data %&amp;gt;%
  ggplot(aes(X, Y)) +
  geom_smooth(aes(colour = vowel), method = &amp;quot;loess&amp;quot;) +
  coord_fixed() +
  facet_grid(c2 ~ vowel) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2018-08-23-plotting-tongue-contours-with-ggplot2_files/figure-html/smooth-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can immediately notice that with /u/ there is something odd going on. That does not look like a tongue surface (maybe that of a chameleon! Definitely not one of a ‘hooman’.) The smooths for /a/ and /o/ seem quite standard.&lt;/p&gt;
&lt;p&gt;To see what is going on, let’s plot now also the individual points as recorded in the data, whith a superimoposed smooth, for comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tongue_data %&amp;gt;%
  ggplot(aes(X, Y)) +
  geom_point(alpha = 0.1) +
  geom_smooth(aes(colour = vowel), method = &amp;quot;loess&amp;quot;) +
  coord_fixed() +
  facet_grid(c2 ~ vowel) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2018-08-23-plotting-tongue-contours-with-ggplot2_files/figure-html/points-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the smooths with /a/ and /o/ more or less have a good fit when compared to the points, with /u/ the smooths are really off.&lt;/p&gt;
&lt;p&gt;This happpens because the tongue root (in this particular case) developpes vertically rather than slanted. The smooth isagnostic about the fact that points lying on the same X value but with different Y values belong to different portion of the tongue contour. The result is that smoothing happens across tongue parts.&lt;/p&gt;
&lt;p&gt;An alternative (if you don’t like points) is to use &lt;code&gt;geom_path()&lt;/code&gt; to plot the individual tongue contours as lines. &lt;code&gt;geom_path()&lt;/code&gt; connects points with a line, following the order in which they appear in the dataset. So, before using this geometry, we need to arrange the dataframe such that the points are in the right order (now they are in the wrong order).&lt;/p&gt;
&lt;p&gt;To do so, we can use &lt;code&gt;rec_date&lt;/code&gt; (which identifies the individual contours) and &lt;code&gt;fan_line&lt;/code&gt; which indicates the orders of points (for each contour, there a maximum 42 points/fan lines; NAs have been excluded).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tongue_data &amp;lt;- tongue_data %&amp;gt;%
  arrange(rec_date, fan_line)

tongue_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,239 x 30
##    speaker seconds rec_date prompt label TT_displacement… TT_velocity
##    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  2 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  3 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  4 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  5 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  6 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  7 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  8 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
##  9 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
## 10 it01       1.11 29/11/2… Dico … max_…             77.4       -7.18
## # … with 1,229 more rows, and 23 more variables: TT_velocity_abs &amp;lt;dbl&amp;gt;,
## #   TD_displacement_sm &amp;lt;dbl&amp;gt;, TD_velocity &amp;lt;dbl&amp;gt;, TD_velocity_abs &amp;lt;dbl&amp;gt;,
## #   TR_displacement_sm &amp;lt;dbl&amp;gt;, TR_velocity &amp;lt;dbl&amp;gt;, TR_velocity_abs &amp;lt;dbl&amp;gt;,
## #   fan_line &amp;lt;int&amp;gt;, X &amp;lt;dbl&amp;gt;, Y &amp;lt;dbl&amp;gt;, word &amp;lt;chr&amp;gt;, language &amp;lt;chr&amp;gt;, sex &amp;lt;chr&amp;gt;,
## #   item &amp;lt;int&amp;gt;, ipa &amp;lt;chr&amp;gt;, c1 &amp;lt;chr&amp;gt;, c1_phonation &amp;lt;chr&amp;gt;, vowel &amp;lt;chr&amp;gt;,
## #   anteropost &amp;lt;chr&amp;gt;, height &amp;lt;chr&amp;gt;, c2 &amp;lt;chr&amp;gt;, c2_phonation &amp;lt;chr&amp;gt;,
## #   c2_place &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use &lt;code&gt;geom_path()&lt;/code&gt;. The argument &lt;code&gt;group = rec_date&lt;/code&gt; ensures that individual lines are plotted (without it, the last point of one contour is connected with the first of the contour following in the dataset).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tongue_data %&amp;gt;%
  ggplot(aes(X, Y)) +
  geom_path(aes(group = rec_date, colour = vowel), alpha = 0.5) +
  coord_fixed() +
  facet_grid(c2 ~ vowel) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2018-08-23-plotting-tongue-contours-with-ggplot2_files/figure-html/path-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The tongue root in /u/ is now properly rendered.&lt;/p&gt;
&lt;p&gt;But what fif you want to plot a single contour (possibly with confidence intervals) for each of the 6 panels in the previous figure, rather than all the contours?&lt;/p&gt;
&lt;p&gt;An option is to plot an average contour (litterally, the aveages of X and Y). We can easily do that by grouping the data by &lt;code&gt;fan_line&lt;/code&gt; and then &lt;code&gt;summarise()&lt;/code&gt; it. Plotting can then be done with &lt;code&gt;geom_path()&lt;/code&gt; and &lt;code&gt;geom_polygon()&lt;/code&gt;. All together, the code looks like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xy_mean &amp;lt;- tongue_data %&amp;gt;%
  group_by(fan_line, vowel, c2) %&amp;gt;%
  summarise(
    X_mean = mean(X, na.rm = TRUE),
    Y_mean = mean(Y, na.rm = TRUE)
  )

xy_ci &amp;lt;- tongue_data %&amp;gt;%
  group_by(fan_line, vowel, c2) %&amp;gt;%
  summarise(
    X_CI_low = t.test(X)$conf.int[1],
    X_CI_up = t.test(X)$conf.int[2],
    Y_CI_low = t.test(Y)$conf.int[1],
    Y_CI_up = t.test(Y)$conf.int[2]
  )

ci_upper &amp;lt;- xy_ci %&amp;gt;%
  dplyr::select(-X_CI_low, -Y_CI_low) %&amp;gt;%
  dplyr::rename(
    CI_X = X_CI_up,
    CI_Y = Y_CI_up
  )

ci_lower &amp;lt;- xy_ci %&amp;gt;%
  dplyr::select(-X_CI_up, -Y_CI_up) %&amp;gt;%
  dplyr::arrange(dplyr::desc(fan_line)) %&amp;gt;%
  dplyr::rename(
    CI_X = X_CI_low,
    CI_Y = Y_CI_low
  )

ci &amp;lt;- rbind(ci_upper, ci_lower)

ggplot(xy_mean, aes(X_mean, Y_mean)) +
  geom_polygon(data = ci, aes(x = CI_X, y = CI_Y), alpha = 0.2) +
  geom_path(aes(X_mean, Y_mean, colour = vowel)) +
  facet_grid(c2 ~ vowel) +
  coord_fixed() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2018-08-23-plotting-tongue-contours-with-ggplot2_files/figure-html/mean-path-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vowel formants trajectories and tidy data</title>
      <link>https://stefanocoretta.github.io/post/vowel-formants-trajectories-and-tidy-data/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://stefanocoretta.github.io/post/vowel-formants-trajectories-and-tidy-data/</guid>
      <description>
&lt;script src=&#34;https://stefanocoretta.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;With the advent of more powerful statistical methods for accessing time series data, it is now more common to compare whole vowel formant trajectories rather then just using average formant values.&lt;/p&gt;
&lt;p&gt;Sometimes, the output of the Praat script used for extracting formats data gives us a ‘wide format’ dataset.
In this format, separate columns contain formant values for each interval within the vowel.
Normally, values are extracted every 10% or 5% intervals within the vowel.&lt;/p&gt;
&lt;p&gt;To work with formant trajectories data in &lt;code&gt;R&lt;/code&gt;, we need instead a ‘long format’ dataset.
In a long format dataset, the percent intervals are layed out in rows, rather then columns.&lt;/p&gt;
&lt;p&gt;We start by reading in the formant data, which were kindly provided by &lt;a href=&#34;http://tiny.cc/sjn&#34;&gt;Stephen Nichols&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trajectories &amp;lt;- read_csv(&amp;quot;./static/data/nichols-2018/tulemupepelako.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   .default = col_double(),
##   Vowel = col_character(),
##   Word = col_character()
## )
## ℹ Use `spec()` for the full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trajectories&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 61
##    Time Vowel Word  Duration F1_05 F1_10 F1_15 F1_20 F1_25 F1_30 F1_35 F1_40
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1  194. u     tule…     43.3  406.  439.  453.  456.  430.  357.  314.  288.
## 2  194. e     tule…    103.   503.  517.  537.  564.  556.  362.  315.  295.
## 3  194. u     tule…     14.1  290.  288.  286.  283.  281.  281.  282.  295.
## 4  194. e     tule…     75.7  440.  441.  439.  429.  386.  269.  250.  318.
## 5  194. e     tule…     68.2  437.  445.  479.  562.  622.  605.  523.  618.
## 6  195. a     tule…     89.8  800.  736.  662.  543.  447.  564.  768.  358.
## 7  195. o     tule…     98.5  482.  463.  471.  326.  316.  573. 1511. 1389.
## # … with 49 more variables: F1_45 &amp;lt;dbl&amp;gt;, F1_50 &amp;lt;dbl&amp;gt;, F1_55 &amp;lt;dbl&amp;gt;, F1_60 &amp;lt;dbl&amp;gt;,
## #   F1_65 &amp;lt;dbl&amp;gt;, F1_70 &amp;lt;dbl&amp;gt;, F1_75 &amp;lt;dbl&amp;gt;, F1_80 &amp;lt;dbl&amp;gt;, F1_85 &amp;lt;dbl&amp;gt;,
## #   F1_90 &amp;lt;dbl&amp;gt;, F1_95 &amp;lt;dbl&amp;gt;, F2_05 &amp;lt;dbl&amp;gt;, F2_10 &amp;lt;dbl&amp;gt;, F2_15 &amp;lt;dbl&amp;gt;,
## #   F2_20 &amp;lt;dbl&amp;gt;, F2_25 &amp;lt;dbl&amp;gt;, F2_30 &amp;lt;dbl&amp;gt;, F2_35 &amp;lt;dbl&amp;gt;, F2_40 &amp;lt;dbl&amp;gt;,
## #   F2_45 &amp;lt;dbl&amp;gt;, F2_50 &amp;lt;dbl&amp;gt;, F2_55 &amp;lt;dbl&amp;gt;, F2_60 &amp;lt;dbl&amp;gt;, F2_65 &amp;lt;dbl&amp;gt;,
## #   F2_70 &amp;lt;dbl&amp;gt;, F2_75 &amp;lt;dbl&amp;gt;, F2_80 &amp;lt;dbl&amp;gt;, F2_85 &amp;lt;dbl&amp;gt;, F2_90 &amp;lt;dbl&amp;gt;,
## #   F2_95 &amp;lt;dbl&amp;gt;, F3_05 &amp;lt;dbl&amp;gt;, F3_10 &amp;lt;dbl&amp;gt;, F3_15 &amp;lt;dbl&amp;gt;, F3_20 &amp;lt;dbl&amp;gt;,
## #   F3_25 &amp;lt;dbl&amp;gt;, F3_30 &amp;lt;dbl&amp;gt;, F3_35 &amp;lt;dbl&amp;gt;, F3_40 &amp;lt;dbl&amp;gt;, F3_45 &amp;lt;dbl&amp;gt;,
## #   F3_50 &amp;lt;dbl&amp;gt;, F3_55 &amp;lt;dbl&amp;gt;, F3_60 &amp;lt;dbl&amp;gt;, F3_65 &amp;lt;dbl&amp;gt;, F3_70 &amp;lt;dbl&amp;gt;,
## #   F3_75 &amp;lt;dbl&amp;gt;, F3_80 &amp;lt;dbl&amp;gt;, F3_85 &amp;lt;dbl&amp;gt;, F3_90 &amp;lt;dbl&amp;gt;, F3_95 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset contains formant values of F1-F3 at 5% intervals for the vowels of the word &lt;em&gt;tulemupepelako&lt;/em&gt; ‘we are praying for her’ (Bemba, [&lt;code&gt;bemb1257&lt;/code&gt;]).
It’s a toy dataset of course, just for purpose of illustration, so not very exciting!&lt;/p&gt;
&lt;p&gt;The values for each 5% interval for each formant are in separate columns.
For plotting and modelling, though, we need the data to be in a ‘tidy’ format &lt;span class=&#34;citation&#34;&gt;(Wickham 2014)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;each observation unit is in a separate row&lt;/li&gt;
&lt;li&gt;each variable is a separate column&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our case, each observation unit is a 5% interval.
As for the variables, they are: time in the recording, word, vowel, vowel duration, F1, F2, and F3.&lt;/p&gt;
&lt;p&gt;The first step towards a long format data set is to gather all the columns that have formant values to a column indicating the formant plus interval (like ‘F1_05’) and the value of that formant in that interval.
We can use &lt;code&gt;gather()&lt;/code&gt; for this.&lt;/p&gt;
&lt;p&gt;In the following chunks, I will illustrate the results of each step separately by saving the output of the functions to variables.
Later in this post, you’ll find a pipe chain with all functions without intermediate outputs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trajectories_gath &amp;lt;- gather(trajectories, &amp;quot;formant&amp;quot;, &amp;quot;value&amp;quot;, F1_05:F3_95)
trajectories_gath&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 399 x 6
##     Time Vowel Word           Duration formant value
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  194. u     tulemupepelako     43.3 F1_05    406.
##  2  194. e     tulemupepelako    103.  F1_05    503.
##  3  194. u     tulemupepelako     14.1 F1_05    290.
##  4  194. e     tulemupepelako     75.7 F1_05    440.
##  5  194. e     tulemupepelako     68.2 F1_05    437.
##  6  195. a     tulemupepelako     89.8 F1_05    800.
##  7  195. o     tulemupepelako     98.5 F1_05    482.
##  8  194. u     tulemupepelako     43.3 F1_10    439.
##  9  194. e     tulemupepelako    103.  F1_10    517.
## 10  194. u     tulemupepelako     14.1 F1_10    288.
## # … with 389 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column &lt;code&gt;formant&lt;/code&gt; with the formant plus interval label, and a column &lt;code&gt;value&lt;/code&gt; with its value (in Hertz in our case).&lt;/p&gt;
&lt;p&gt;The next step is to separate the label of the formant plus interval into two separate columns: one for &lt;code&gt;formant&lt;/code&gt;, and one for &lt;code&gt;interval&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trajectories_sep &amp;lt;- separate(trajectories_gath, formant, c(&amp;quot;formant&amp;quot;, &amp;quot;interval&amp;quot;), convert = TRUE)
trajectories_sep&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 399 x 7
##     Time Vowel Word           Duration formant interval value
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1  194. u     tulemupepelako     43.3 F1             5  406.
##  2  194. e     tulemupepelako    103.  F1             5  503.
##  3  194. u     tulemupepelako     14.1 F1             5  290.
##  4  194. e     tulemupepelako     75.7 F1             5  440.
##  5  194. e     tulemupepelako     68.2 F1             5  437.
##  6  195. a     tulemupepelako     89.8 F1             5  800.
##  7  195. o     tulemupepelako     98.5 F1             5  482.
##  8  194. u     tulemupepelako     43.3 F1            10  439.
##  9  194. e     tulemupepelako    103.  F1            10  517.
## 10  194. u     tulemupepelako     14.1 F1            10  288.
## # … with 389 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create individual columns for each formant from F1 to F3.
We can achieve this with &lt;code&gt;spread()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trajectories_spr &amp;lt;- spread(trajectories_sep, formant, value)
trajectories_spr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 133 x 8
##     Time Vowel Word           Duration interval    F1    F2    F3
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  194. u     tulemupepelako     43.3        5  406. 1205. 2626.
##  2  194. u     tulemupepelako     43.3       10  439. 1226. 2556.
##  3  194. u     tulemupepelako     43.3       15  453. 1246. 2507.
##  4  194. u     tulemupepelako     43.3       20  456. 1291. 2451.
##  5  194. u     tulemupepelako     43.3       25  430. 1418. 2331.
##  6  194. u     tulemupepelako     43.3       30  357. 1555. 2268.
##  7  194. u     tulemupepelako     43.3       35  314. 1603. 2335.
##  8  194. u     tulemupepelako     43.3       40  288. 1709. 2437.
##  9  194. u     tulemupepelako     43.3       45  286. 1712. 2461.
## 10  194. u     tulemupepelako     43.3       50  327. 1747. 2470.
## # … with 123 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we have now is a long format dataset, with separate columns for each formant, and individual rows for each vowel interval.&lt;/p&gt;
&lt;div id=&#34;the-pipe-chain&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The pipe chain&lt;/h2&gt;
&lt;p&gt;All the steps above can be chained by using the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trajectories &amp;lt;- trajectories %&amp;gt;%
  gather(&amp;quot;formant&amp;quot;, &amp;quot;value&amp;quot;, F1_05:F3_95) %&amp;gt;%
  separate(formant, c(&amp;quot;formant&amp;quot;, &amp;quot;interval&amp;quot;), convert = TRUE) %&amp;gt;%
  spread(formant, value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can finally easily plot the formant trajectories.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trajectories %&amp;gt;%
  ggplot(aes(x = interval)) +
  geom_smooth(aes(y = F1), se = FALSE, colour = &amp;quot;red&amp;quot;) +
  geom_smooth(aes(y = F2), se = FALSE, colour = &amp;quot;green&amp;quot;) +
  geom_smooth(aes(y = F3), se = FALSE, colour = &amp;quot;blue&amp;quot;) +
  facet_wrap(~ Vowel) +
  ylab(&amp;quot;Hertz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stefanocoretta.github.io/post/2018-03-02-vowel-formants-trajectories-and-tidy-data_files/figure-html/trajectory-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-wickham2014&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2014. &lt;span&gt;“Tidy Data.”&lt;/span&gt; &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 59 (10): 1–23.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
